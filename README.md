# Awesome-Long2short-on-LRMs

Awesome-Long2short-on-LRMs is a collection of state-of-the-art, novel, exciting **long2short** methods on **large reasoning models**. It contains papers, codes, datasets, evaluations, and analyses.

## Training Based
| Time | Title                                                      |  Venue  |                           Paper                            |                            Code                            |
| ---- | -------------------------------------------------------- | :-----: | :-------------------------------------------------------: | :-------------------------------------------------------: |
| 2025.03 | **L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning** | arXiv | [link](https://www.arxiv.org/pdf/2503.04697) | [link](https://github.com/cmu-l3/l1) |
| 2025.02 | **Self-Training Elicits Concise Reasoning in Large Language Models** | arXiv | [link](https://arxiv.org/pdf/2502.20122) | [link](https://github.com/TergelMunkhbat/concise-reasoning) |
| 2025.02 | **TokenSkip: Controllable Chain-of-Thought Compression in LLMs** |   arXiv     | [link](https://arxiv.org/abs/2502.12067) |        [link](https://github.com/hemingkx/TokenSkip)     |
| 2025.02 | **LightThinker: Thinking Step-by-Step Compression** |   arXiv     | [link](https://arxiv.org/abs/2502.15589) |        [link](https://github.com/zjunlp/LightThinker)      |
| 2025.01 | **Kimi k1.5: Scaling Reinforcement Learning with LLMs** |   arXiv     | [link](https://arxiv.org/abs/2501.12599) |        -     
| 2024.12 | **Token-Budget-Aware LLM Reasoning** | arXiv | [link](https://arxiv.org/abs/2412.18547) | [link](https://github.com/GeniusHTX/TALE) |


## Training Free
| Time | Title                                                      |  Venue  |                           Paper                            |                            Code                            |
| ---- | -------------------------------------------------------- | :-----: | :-------------------------------------------------------: | :-------------------------------------------------------: |
| 2025.03 | **Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching** | arXiv | [link](https://arxiv.org/pdf/2503.05179v1) | [link](https://github.com/SimonAytes/SoT) |
| 2025.02 | **Chain of Draft: Thinking Faster by Writing Less** |   arXiv     | [link](https://arxiv.org/pdf/2502.18600) |        [link](https://github.com/sileix/chain-of-draft)      |
| 2025.02 | **Reward-Guided Speculative Decoding for Efficient LLM Reasoning** |   arXiv     | [link](https://arxiv.org/pdf/2501.19324) |        [link](https://github.com/BaohaoLiao/RSD)      |
| 2024.12 | **Token-Budget-Aware LLM Reasoning** | arXiv | [link](https://arxiv.org/abs/2412.18547) | [link](https://github.com/GeniusHTX/TALE) |
| 2024.12 | **Efficiently Serving LLM Reasoning Programs with Certaindex** | arXiv | [link](https://arxiv.org/abs/2412.20993) | - |






## Contributors
<a href="https://github.com/Hongcheng-Gao" target="_blank"><img src="https://avatars.githubusercontent.com/u/96536860?v=4" alt="Hongcheng-Gao" width="72" height="72"/></a> 
<a href="https://github.com/xinlong-yang" target="_blank"><img src="https://avatars.githubusercontent.com/u/73691354?v=4" alt="xinlong-yang" width="72" height="72"/></a> 
<a href="https://github.com/yueliu1999" target="_blank"><img src="https://avatars.githubusercontent.com/u/41297969?s=64&v=4" alt="yueliu1999" width="72" height="72"/></a> 
<a href="https://github.com/ColorDavid" target="_blank"><img src="https://avatars.githubusercontent.com/u/57055043?v=4" alt="ColorDavid" width="72" height="72"/></a> 
<a href="https://github.com/junming-yang" target="_blank"><img src="https://avatars.githubusercontent.com/u/60545459?v=4" alt="junming-yang" width="72" height="72"/></a> 
























